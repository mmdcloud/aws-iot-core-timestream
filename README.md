# IoT Real-Time Data Pipeline on AWS

[![Terraform](https://img.shields.io/badge/Terraform-1.0+-623CE4?logo=terraform)](https://www.terraform.io/)
[![AWS](https://img.shields.io/badge/AWS-Cloud-FF9900?logo=amazon-aws)](https://aws.amazon.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

A production-grade, fully automated IoT data ingestion and analytics pipeline built on AWS using Terraform. This infrastructure enables real-time streaming, transformation, and analysis of IoT device data at scale.

## üèóÔ∏è Architecture Overview

```
IoT Devices (Simulated EC2)
    ‚Üì
AWS IoT Core (MQTT)
    ‚Üì
Kinesis Data Streams
    ‚Üì
Lambda (Transformation)
    ‚Üì
‚îú‚îÄ‚Üí Amazon Timestream (Time-series DB)
‚îî‚îÄ‚Üí S3 (Data Lake)
    ‚Üì
AWS Glue Crawler
    ‚Üì
AWS Glue Data Catalog
    ‚Üì
Amazon Athena (SQL Analytics)
```

## ‚ú® Features

- **Fully Managed IoT Infrastructure**: Secure device connectivity via AWS IoT Core with certificate-based authentication
- **Real-Time Data Streaming**: High-throughput data ingestion using Kinesis Data Streams with on-demand scaling
- **Serverless Transformation**: Lambda-based data processing with configurable business logic
- **Time-Series Storage**: Amazon Timestream for efficient storage and querying of time-series IoT data
- **Data Lake Integration**: Automatic archival to S3 for historical analysis and compliance
- **Automated Data Cataloging**: AWS Glue crawlers for schema discovery and metadata management
- **SQL Analytics**: Ad-hoc querying via Amazon Athena with S3-backed storage
- **Infrastructure as Code**: 100% Terraform-managed infrastructure with modular design
- **Production-Ready Security**: VPC isolation, security groups, IAM least-privilege policies, and encrypted data transport

## üìã Prerequisites

- **Terraform**: >= 1.0.0
- **AWS Account**: With appropriate permissions
- **AWS CLI**: Configured with credentials (`aws configure`)
- **SSH Key Pair**: For EC2 instance access (referenced as `madmaxkeypair`)
- **Python 3.12**: For Lambda function development (if modifying transformation logic)

## üöÄ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/your-org/iot-data-pipeline.git
cd iot-data-pipeline
```

### 2. Configure Variables

Create a `terraform.tfvars` file:

```hcl
# Network Configuration
azs             = ["us-east-1a", "us-east-1b"]
public_subnets  = ["10.0.1.0/24", "10.0.2.0/24"]
private_subnets = ["10.0.11.0/24", "10.0.12.0/24"]

# Glue Configuration
glue_database_name = "iot_database"
glue_table_name    = "iot_table"
glue_crawler_name  = "iot_crawler"

# AWS Region
aws_region = "us-east-1"
```

### 3. Prepare Lambda Function Code

Package your Lambda transformation function:

```bash
cd lambda
zip -r lambda.zip lambda.py
aws s3 cp lambda.zip s3://transform-function-code-bucket-<random-id>/
cd ..
```

### 4. Initialize and Deploy

```bash
# Initialize Terraform
terraform init

# Review the execution plan
terraform plan

# Deploy infrastructure
terraform apply
```

### 5. Verify Deployment

```bash
# Get IoT endpoint
aws iot describe-endpoint --endpoint-type iot:Data-ATS

# Check Kinesis stream
aws kinesis describe-stream --stream-name kinesis-stream

# Verify Timestream database
aws timestream-query query --query-string "SELECT * FROM iot_timestream_db LIMIT 10"
```

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ main.tf                    # Main infrastructure configuration
‚îú‚îÄ‚îÄ variables.tf               # Input variables
‚îú‚îÄ‚îÄ outputs.tf                 # Output values
‚îú‚îÄ‚îÄ terraform.tfvars           # Variable values (gitignored)
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ vpc/                   # VPC and networking module
‚îÇ   ‚îú‚îÄ‚îÄ security-groups/       # Security group module
‚îÇ   ‚îú‚îÄ‚îÄ ec2/                   # EC2 instance module
‚îÇ   ‚îú‚îÄ‚îÄ s3/                    # S3 bucket module
‚îÇ   ‚îú‚îÄ‚îÄ kinesis/               # Kinesis stream module
‚îÇ   ‚îú‚îÄ‚îÄ lambda/                # Lambda function module
‚îÇ   ‚îú‚îÄ‚îÄ iam/                   # IAM roles and policies module
‚îÇ   ‚îî‚îÄ‚îÄ timestream/            # Timestream database module
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ user_data.sh           # EC2 instance initialization script
‚îú‚îÄ‚îÄ lambda/
‚îÇ   ‚îî‚îÄ‚îÄ lambda.py              # Lambda transformation logic
‚îî‚îÄ‚îÄ README.md                  # This file
```

## üîß Configuration

### IoT Device Simulation

The EC2 instance automatically configures an MQTT client on boot:

```bash
# User data script provisions:
- AWS IoT SDK for Python
- Device certificates and private keys
- MQTT connection to IoT Core endpoint
- Publishes sample telemetry data to topic/mqtt
```

### Data Transformation

Customize Lambda function (`lambda/lambda.py`):

```python
import json
import base64

def lambda_handler(event, context):
    """
    Transform incoming IoT data before storage
    """
    output = []
    
    for record in event['Records']:
        # Decode Kinesis data
        payload = base64.b64decode(record['kinesis']['data'])
        data = json.loads(payload)
        
        # Apply transformations
        transformed = {
            'timestamp': data['timestamp'],
            'device_id': data['deviceId'],
            'temperature': float(data['temperature']),
            'humidity': float(data['humidity']),
            # Add custom business logic here
        }
        
        output.append({
            'recordId': record['recordId'],
            'result': 'Ok',
            'data': base64.b64encode(
                json.dumps(transformed).encode('utf-8')
            ).decode('utf-8')
        })
    
    return {'records': output}
```

### Kinesis Stream Configuration

Modify stream settings in `modules/kinesis`:

```hcl
# On-demand scaling (default)
stream_mode = "ON_DEMAND"

# Or use provisioned mode for predictable workloads
stream_mode      = "PROVISIONED"
shard_count      = 2
retention_period = 168  # 7 days
```

### Timestream Retention Policies

Configure data lifecycle in `modules/timestream`:

```hcl
memory_store_retention_period_in_hours  = 24    # Hot storage
magnetic_store_retention_period_in_days = 365   # Cold storage
```

## üîê Security Considerations

### Certificate Management

- Device certificates are auto-generated using Terraform's `tls` provider
- Certificates embedded in EC2 user data (for demo purposes)
- **Production**: Use AWS Secrets Manager or Parameter Store for certificate distribution

### Network Security

- IoT devices in public subnets with restricted security groups
- Timestream in private subnets (no direct internet access)
- Security group rules follow principle of least privilege

### IAM Policies

All IAM roles follow least-privilege access:

```hcl
# IoT Core ‚Üí Kinesis
- kinesis:PutRecord, kinesis:PutRecords (scoped to specific stream)

# Lambda ‚Üí CloudWatch Logs
- logs:CreateLogGroup, logs:CreateLogStream, logs:PutLogEvents

# Glue Crawler ‚Üí S3
- s3:GetObject, s3:ListBucket (scoped to specific bucket)
```

### Data Encryption

- **In Transit**: TLS 1.2+ for all AWS service communications
- **At Rest**: 
  - S3 buckets with default SSE-S3 encryption
  - Kinesis with AWS-managed keys
  - Timestream with AWS-managed encryption

## üìä Monitoring and Observability

### CloudWatch Metrics

Key metrics to monitor:

```
IoT Core:
- PublishIn.Success
- RulesExecuted
- Failure (rule execution failures)

Kinesis:
- IncomingRecords
- IncomingBytes
- GetRecords.IteratorAgeMilliseconds
- WriteProvisionedThroughputExceeded

Lambda:
- Invocations
- Errors
- Duration
- Throttles

Timestream:
- SystemErrors
- UserErrors
- SuccessfulRequestLatency
```

### CloudWatch Alarms (Recommended)

```hcl
# Add to main.tf
resource "aws_cloudwatch_metric_alarm" "kinesis_iterator_age" {
  alarm_name          = "kinesis-high-iterator-age"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "GetRecords.IteratorAgeMilliseconds"
  namespace           = "AWS/Kinesis"
  period              = "300"
  statistic           = "Average"
  threshold           = "60000"  # 1 minute
  alarm_description   = "Kinesis consumer is falling behind"
}
```

### Logging

- **IoT Core**: CloudWatch Logs for rule execution
- **Lambda**: Automatic CloudWatch Logs integration
- **VPC Flow Logs**: Enable for network traffic analysis (optional)

## üß™ Testing

### Send Test Data via IoT Core

```bash
# Publish test message
aws iot-data publish \
  --topic topic/mqtt \
  --cli-binary-format raw-in-base64-out \
  --payload '{"deviceId":"test-001","temperature":23.5,"humidity":65,"timestamp":"2024-12-19T10:30:00Z"}'
```

### Query Timestream Data

```bash
aws timestream-query query \
  --query-string "SELECT * FROM iot_timestream_db.iot_table WHERE time > ago(1h)"
```

### Query Athena

```bash
aws athena start-query-execution \
  --query-string "SELECT * FROM iot_database.iot_table LIMIT 100" \
  --result-configuration OutputLocation=s3://athena-temp-results-bucket-<id>/
```

## üìà Scaling Considerations

### Kinesis Scaling

- **On-Demand Mode**: Automatically scales to 200 MB/s write, 400 MB/s read per shard
- **Provisioned Mode**: Manual shard management for cost optimization

### Lambda Concurrency

```hcl
# Add reserved concurrency to prevent throttling
reserved_concurrent_executions = 100
```

### Timestream Performance

- Memory store: Sub-second query latency
- Magnetic store: Higher latency but cost-effective
- Use time-based partitioning for optimal query performance

## üí∞ Cost Optimization

### Estimated Monthly Costs (us-east-1)

| Service | Usage | Cost |
|---------|-------|------|
| IoT Core | 1M messages/month | $1.00 |
| Kinesis (On-Demand) | 1GB/day ingestion | $35.04 |
| Lambda | 1M invocations | $0.20 |
| Timestream | 1GB writes, 10GB storage | $8.50 |
| S3 | 100GB storage | $2.30 |
| Athena | 10GB scanned | $0.50 |
| **Total** | | **~$47.54/month** |

### Cost Reduction Tips

1. Use Kinesis Provisioned mode for steady workloads
2. Enable S3 Intelligent-Tiering for long-term storage
3. Configure Timestream magnetic store for historical data
4. Set S3 lifecycle policies to Glacier after 90 days
5. Use Athena partitioning to reduce scan size

## üîÑ CI/CD Integration

### GitHub Actions Example

```yaml
name: Terraform Deploy

on:
  push:
    branches: [ main ]

jobs:
  terraform:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        
      - name: Terraform Init
        run: terraform init
        
      - name: Terraform Plan
        run: terraform plan
        
      - name: Terraform Apply
        if: github.ref == 'refs/heads/main'
        run: terraform apply -auto-approve
```

## üßπ Cleanup

To destroy all resources:

```bash
terraform destroy
```

**Warning**: This will permanently delete:
- All S3 data
- Timestream database and tables
- Kinesis streams
- IoT devices and certificates

## üêõ Troubleshooting

### Issue: IoT Device Not Publishing

```bash
# Check IoT Core logs
aws logs tail /aws/iot/logs --follow

# Verify certificate is active
aws iot describe-certificate --certificate-id <cert-id>

# Test connectivity
aws iot-data publish --topic test --payload '{"test":true}'
```

### Issue: Lambda Not Processing Records

```bash
# Check Lambda logs
aws logs tail /aws/lambda/transform-function --follow

# Verify event source mapping
aws lambda list-event-source-mappings --function-name transform-function

# Check IAM permissions
aws iam simulate-principal-policy \
  --policy-source-arn <role-arn> \
  --action-names kinesis:GetRecords
```

### Issue: High Kinesis Iterator Age

- Increase Lambda batch size
- Add reserved concurrency to Lambda
- Scale out Lambda execution (parallel processing)
- Check for processing errors in CloudWatch Logs

## üìö Additional Resources

- [AWS IoT Core Documentation](https://docs.aws.amazon.com/iot/)
- [Kinesis Best Practices](https://docs.aws.amazon.com/streams/latest/dev/best-practices.html)
- [Timestream Developer Guide](https://docs.aws.amazon.com/timestream/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add new feature'`)
4. Push to branch (`git push origin feature/improvement`)
5. Create a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üë• Support

For issues and questions:
- Open a GitHub Issue
- Contact: devops@yourcompany.com
- Slack: #iot-platform

## üó∫Ô∏è Roadmap

- [ ] Add DynamoDB integration for device state management
- [ ] Implement SageMaker for anomaly detection
- [ ] Add QuickSight dashboards for visualization
- [ ] Support multi-region deployment
- [ ] Add Kinesis Firehose for direct S3 delivery
- [ ] Implement AWS CDK version
- [ ] Add comprehensive end-to-end tests

---

**Built with ‚ù§Ô∏è by the DevOps Team**
